# Default values for monitoring-stack 
nameOverride: ""
fullnameOverride: ""

# Namespace
namespace:
  name: monitoring
  create: true

# Prometheus configuration
prometheus:
  enabled: true
  image:
    repository: prom/prometheus
    tag: v3.5.0
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 8080
    targetPort: 9090
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '9090'
  
  resources:
    requests:
      cpu: 200m
      memory: 1000Mi
    limits:
      cpu: 1000m
      memory: 2000Mi
  
  # Prometheus configuration
  config:
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    scrape_configs:
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
      
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https #kubernetes vahetatud "katki" api testiks
      
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics
      
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
      
      - job_name: 'kubernetes-endpoints'
        kubernetes_sd_configs:
        - role: endpoints
        relabel_configs:
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_service_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_service_name]
          action: replace
          target_label: kubernetes_service_name

      
      - job_name: 'node-exporter'
        static_configs:
          - targets: ['monitoring-stack-node-exporter.monitoring.svc.cluster.local:9100']
      
      - job_name: 'kubelet-cadvisor'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      
      - job_name: 'metrics-backend'
        static_configs:
          - targets: ['metrics-backend.monitoring.svc.cluster.local:3000']
        metrics_path: /metrics/prometheus
        scrape_interval: 15s
      

      - job_name: 'elasticsearch'
        static_configs:
          - targets: ['elasticsearch.monitoring.svc.cluster.local:9114']
        metrics_path: /metrics
        scrape_interval: 30s
        
      - job_name: 'fluent-bit'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: fluent-bit
          - source_labels: [__address__]
        action: replace
        target_label: __address__
        regex: (.+):.*
        replacement: ${1}:2021  # Metrics port   

    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - "monitoring-stack-alertmanager.monitoring.svc.cluster.local:9093"

  # Storage configuration
  storage:
    retention: 200h
  
  persistence:
    enabled: true
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce
    size: "10Gi" # Prometheus data can be large, so a larger size is appropriate

# Grafana configuration
grafana:
  enabled: true
  image:
    repository: grafana/grafana
    tag: 12.1.0
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 3000
    targetPort: 3000
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '3000'
  
  # Admin credentials
  adminUser: admin
  adminPassword: admin
  
  # Security settings
  security:
    allowSignUp: false
  
  resources:
    requests:
      cpu: 500m
      memory: 500Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  
  persistence:
    enabled: true
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce
    size: "10Gi"
  
  # Datasources
  datasources:
    prometheus:
      name: prometheus
      type: prometheus
      url: "http://{{ include \"monitoring-stack.prometheus.fullname\" . }}.{{ .Values.namespace.name }}.svc.cluster.local:{{ .Values.prometheus.service.port }}"
      access: proxy
      isDefault: true

# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    prometheus.io/scrape: "true"
    prometheus.io/port: "80"
  
  grafana:
    enabled: true
    host: grafana.local
    path: /
    pathType: Prefix
  
  prometheus:
    enabled: true
    host: prometheus.local
    path: /
    pathType: Prefix
  
  kibana:
    enabled: true
    host: kibana.local
    path: /
    pathType: Prefix

# Service Account
serviceAccount:
  create: true
  annotations: {}
  # name: "monitoring-stack-kube-state-metrics" # Removed to avoid conflict with kube-state-metrics's own service account

# RBAC
rbac:
  create: true

# Additional monitoring components
kubeStateMetrics:
  enabled: true
  image:
    repository: registry.k8s.io/kube-state-metrics/kube-state-metrics
    tag: v2.16.0
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
    annotations: {}
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

nodeExporter:
  enabled: true
  image:
    repository: prom/node-exporter
    tag: v1.6.1
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 9100
    targetPort: 9100
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '9100'
  
  resources:
    requests:
      cpu: 100m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 128Mi

cAdvisor:
  enabled: false
  image:
    repository: gcr.io/cadvisor/cadvisor
    tag: v0.47.2
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 8080
    targetPort: 8080
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '8080'
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

# Alertmanager configuration
alertmanager:
  enabled: true
  image:
    repository: prom/alertmanager
    tag: v0.26.0
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 9093
    targetPort: 9093
    annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/port: '9093'
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  
  persistence:
    enabled: true
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce
    size: "1Gi" # Alertmanager data is usually small
  
  config:
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alertmanager@example.com'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'web.hook'
    
    receivers:
      - name: 'web.hook'
        webhook_configs:
          - url: 'http://127.0.0.1:5001/'

# Kibana configuration
kibana:
  enabled: true
  image:
    repository: docker.elastic.co/kibana/kibana
    tag: 8.19.0
    pullPolicy: IfNotPresent
  
  service:
    type: ClusterIP
    port: 5601
   
  resources:
    requests:
      cpu: 500m
      memory: 500Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  
  config:
    server.host: "0.0.0.0"
    elasticsearch.hosts: '["http://elasticsearch.monitoring.svc.cluster.local:9200"]'
  
  persistence:
    enabled: true
    storageClassName: "standard"
    accessModes:
      - ReadWriteOnce
    size: "10Gi"

# Fluent-bit configuration
fluent-bit:
  enabled: true
  image:
    repository: fluent/fluent-bit
    tag: 4.0.5
    pullPolicy: IfNotPresent
  
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi
  
  config:
    service: |
      [SERVICE]
          Flush         1
          Log_Level     info
          Daemon        off
          Parsers_File  parsers.conf
          HTTP_Server   On
          HTTP_Listen   0.0.0.0
          HTTP_Port     2020

    inputs: |
      [INPUT]
          Name              tail
          Path              /var/log/containers/*.log
          Tag               kube.*
          Parser            docker
          DB                /var/log/flb_kube.db
          Mem_Buf_Limit     5MB
          Skip_Long_Lines   On
          Refresh_Interval  10

    filters: |
      [FILTER]
          Name                kubernetes
          Match               kube.*
          Kube_URL            https://kubernetes.default.svc:443
          Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
          Kube_Tag_Prefix     kube.var.log.containers.
          Merge_Log           On
          Merge_Log_Key       log_processed
          K8S-Logging.Parser  On
          K8S-Logging.Exclude Off

    outputs: |
      [OUTPUT]
          Name            es
          Match           *
          Host            elasticsearch.monitoring.svc.cluster.local
          Port            9200
          Logstash_Format Off
          Logstash_Prefix kubernetes_logs
          Time_Key        @timestamp
          Replace_Dots    On
          Suppress_Type_Name On

      [OUTPUT]
          Name   prometheus_exporter
          Match  *
          Host   0.0.0.0
          Port   2021

    parsers: |
      [PARSER]
          Name   docker
          Format json
          Time_Key time
          Time_Format %Y-%m-%dT%H:%M:%S.%L
          Time_Keep On
